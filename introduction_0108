표본에 어떤 관측치가 포함되느냐에 따라 평균값이 달라진다는 점에서 표본평균(X̄)은 하나의 확률변수(random variable)가 된다. 그렇게 때문에 X̄의 표준편차(즉 표준오차, std.err.)를 추정할 수 있고, 그것을 이용하여 신뢰구간도 구할 수 있다.

*
 표본평균 sample mean(확률분포를 갖는다 -> 확률변수다)
 표본분산 sample variance
 표본표준편차 sample standard deviation

 표본평균의 평균
 표본평균의 분산
 표본평균의 표준편차(표준오차 standard error)

* 회귀분석: 원인이 결과에 주는 영향을 알기 위한 방법/ 산포도나 상관계수로도 관련성의 정도를 알 수는 있다. 그러나 회귀분석을 사용하면 원인이 결과에 주는 영향을 추정하여 결과를 예측할 수 있다.
데이터를 가장 잘 대변하는 선을 찾는
OLS(Ordinary Least Squares): 잔차(residuals)의 제곱합을 최소화 시키는
최대우도법(Likelihood): 우도(관측된 데이터를 얻을 수 있는 확률 같은 것)가 최대가 되도록 하는

SST(sum of squared total): 총변동
SSR(sum of squared regression): 설명된 변동
SSE(sum of squared error): 설명되지 않은 변동

1. 단순선형회귀모형(simple linear regression model)
 yi = β0 + β1xi + ϵi
 오차항은 변수 x 이외의 요인이 변수 y에 주는 영향을 나타냄, 확률변수로 취급됨
 1) 상수항이 있는 선형회귀모형: SST=SSR+SSE 항상 성립
 2) 상수항이 없는 선형회귀모형: SST=SSR+SSE 성립하지 않음
 상수항이 있는 모형과 없는 모형은 R^2 계산 방법이 서로 다르기 때문에 이것으로 적합도를 비교하는 오류를 범하지 말아야 한다.

2. 다중선형회귀모형(multiple linear regression model): 설명변수가 두개 이상인 경우
 yi = β0 + β1xi1 + β2xi2 + ... + βpxip + ϵi

3. 적합값과 잔차
  ŷ = β0hat + β1hatx1 + β2hatx2 + ... + βkhatxk
  잔차 ehat = y - yhat, 오차 e의 추정량으로 사용됨
  선형회귀모형에서 오차는 관측되지 않는데 비해, 잔차는 관찰할 수 있고 계산히 가능하기 때문에 선형회귀모형의 사후분석에 많이 사용됨
 
 4. 결정계수
  R^2 = 예측값으로 설명된 변동/전변동
  - 전변동: (y i - y bar)^2 합
  - 예측값으로 설명된 변동: (y hat - y bar)^2 합

5. VIF(Variance Inflation Factor, 분산확대요인): 다중공선성을 발견하기 위한 지표, 회귀계수의 분산(표준오차)이 얼마나 커지는지를 나타냄
 나머지 인자와 어떻게 연관이 되는지를 계산할 수 있는 통계량
 VIF가 10보다 클 때는 변수를 제외시키거나 합성하는 등의 대응이 필요
 - 다중공선성: 설명변수 사이에 강한 상관관계가 존재하는 경우

6. 변수선택법
 1) 감소법(Backward): 모든 변수를 넣은 상태에서 기준이 충족되지 않아 p값이 가장 높은(가장 연관이 없는) 변수를 제거
                     AIC(Akaike's An Information Criterion) 감소하다가 증가하기 시작한 순간의 직전 모델 택함
 2) 증가법(Forward): 모든 설명변수의 p값을 조사해 기준을 충족시키고, p값이 가장 낮은 변수를 회귀식에 적용
                    AIC(Akaike's An Information Criterion) 감소하다가 증가하기 시작한 순간의 직전 모델 택함
 -> 감소법, 증가법의 단점: 앞뒤로 AIC 값 더 작아지는 경우가 있을 수 있는데, 그 경우는 배제되어 버림
 3) 감증법: 감소법을 이용해 제외시켜야 할 변수를 정해 재추정, 회귀식에서 제외된 변수에 대해 증가법 적용
 4) 증감법: 증가법에 의해 회귀식에 넣을 변수를 정해 재추정, 회귀식에 넣은 변수에 대해 감소법 적용
 5) All Subset Regression
 
**7. 회귀모형의 적절성: 회귀모형을 가지고 나머지 결과 예측할 수 있다고 이야기 하기 위해서 다음과 같은 조건 확인해보아야 함
  1) 잔차의 정규성: 잔차들이 정규분포 해야한다/ normal q-q 그래프의 직선 위에 점들이 올라와있는지
                 -> if not, 데이터 변환해서 정규성 만족하도록 반응변수의 변환, power transform, Y^lambda/ 비모수
  2) 독립성: 종속변수들끼리 서로 영향을 미치지 않아야 함
  3) 선형성: 분포가 가급적이면 아무 의미 없게/ 임의의 특성을 가지고 있으면 아니 됨/ Residuals vs fitted 그래프에서 특정 모양 띄면 아니 됨
  4) 등분산성
                 -> if not, 
  5) 잔차의 특정 영향력
  
 8. 다항회귀(Polynomial Regression)
 
 9. 더미변수를 이용한 회귀분석: 독립변수가 서열척도나 명목철도인 자료를 분석하고 싶다면 0과 1로 구성된 가상변수인 더미변수로 변경한 후에 분석
 
 
 
 * 비모수 검정
 1. 질적 데이터의 경우: 피어슨 카이제곱검정, 피셔 정확성검정
 2. 극단적인 값이 있는 경우
 
 * 확률변수: 그 값을 취하는 확률이 정해져 있는 변수를 말하는 것으로 양적 척도로 측정되어 있어야 한다.
 * CLT: 개별 데이터의 모집단이 정규분포하지 않아도 거기서 추출한 표본이 충분히 크면 표본평균은 정규분포한다는 것을 보증한다.
 
 * 주성분분석(정보를 수집한다)
 * 인자분석(잠재적인 요인을 찾는다): 변수의 배후에 공통으로 존재하는 공통인자를 추출해 변수 간의 관련성을 이해할 수 있다. 

 * 불편추정량(unbiased estimator): 추정량의 기댓값이 모수와 같아지는 추정량

 * 유의수준(alpha)
 * 유의확률(p-value)
